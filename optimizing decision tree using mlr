data=read.csv("attrition.v1.csv",header=T,sep=",")
data
library(rpart)
library(rpart.plot)

str(data)

data$LDB=factor(data$LDB)
data$supervisors=factor(data$supervisors)
data$sup=factor(data$sup)
data$Year=factor(data$Year)
names(data)
data=data[,c("Location","Designation","Gender","age.joining","Marital.Status",
"Epiexp","Prevexp","Exp.category","Qualification.type","LDB","Salary","Year","sup","Reason")]


set.seed(25)
smp_size=floor(0.70*nrow(data))
train_ind=sample(seq_len(nrow(data)),size=smp_size)

train=data[train_ind,]
test=data[-train_ind,]



library(mlr)

trainTask <- makeClassifTask(data = train,target = "Reason")
testTask <- makeClassifTask(data = test, target = "Reason")


library(FSelector)
#Feature importance
im_feat <- generateFilterValuesData(trainTask, method = c("information.gain","chi.squared"))
plotFilterValues(im_feat,n.show = 20)

getParamSet("classif.rpart")

#make tree learner
makeatree <- makeLearner("classif.rpart", predict.type = "response")

#set 3 fold cross validation
set_cv <- makeResampleDesc("CV",iters = 3L)

gs <- makeParamSet(
makeIntegerParam("minsplit",lower = 10, upper = 50),
makeIntegerParam("minbucket", lower = 5, upper = 50),
makeNumericParam("cp", lower = 0.001, upper = 0.2)
)

gscontrol <- makeTuneControlGrid()

#hypertune the parameters
stune <- tuneParams(learner = makeatree,
resampling = set_cv, task = trainTask, par.set = gs,
control = gscontrol, measures = acc)


stune$x
stune$y

t.tree <- setHyperPars(makeatree, par.vals = stune$x)

#train the model
t.rpart <- train(t.tree, trainTask)
getLearnerModel(t.rpart)

#make predictions
tpmodel <- predict(t.rpart, testTask)
tpmodel$data$response

table(test$Reason,tpmodel$data$response)
